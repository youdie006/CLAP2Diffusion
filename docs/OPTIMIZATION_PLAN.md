# CLAP2Diffusion ìµœì í™” ê³„íš

## ğŸ“‹ í•µì‹¬ ì›ì¹™
1. **ê¸°ì¡´ ì½”ë“œ ë³´ì¡´** - ì‘ë™í•˜ëŠ” ì½”ë“œëŠ” ê±´ë“œë¦¬ì§€ ì•Šê¸°
2. **ì ì§„ì  ì ìš©** - í•œ ë²ˆì— í•˜ë‚˜ì”© í…ŒìŠ¤íŠ¸
3. **Fallback ì¤€ë¹„** - ë¬¸ì œ ë°œìƒ ì‹œ ì¦‰ì‹œ ì›ë³µ ê°€ëŠ¥
4. **ê²€ì¦ ìš°ì„ ** - ì„±ëŠ¥ í–¥ìƒë³´ë‹¤ ì•ˆì •ì„± ìš°ì„ 

## ğŸš¨ ì´ì „ ë¬¸ì œì  ê¸°ë°˜ ì£¼ì˜ì‚¬í•­

### 1. Dtype ì¼ê´€ì„± (ìµœìš°ì„ )
```python
# âŒ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¨ ì½”ë“œ
model_pred = unet(latents, timesteps, encoder_hidden_states)  # Mixed dtype

# âœ… ì•ˆì „í•œ ì ‘ê·¼
model_pred = unet(latents.to(dtype), timesteps, encoder_hidden_states.to(dtype))
loss = F.mse_loss(model_pred.float(), target.float())  # Always float for loss
```

### 2. Dimension Matching
```python
# âŒ ë¬¸ì œ: 512 vs 768 ì°¨ì› ë¶ˆì¼ì¹˜
audio_embeddings = clap_encoder(audio)  # 512 dim
attention_input = audio_embeddings  # Error!

# âœ… í•´ê²°: ëª…ì‹œì  projection
audio_embeddings = clap_encoder(audio)  # 512 dim
audio_tokens = audio_adapter(audio_embeddings)  # 512 -> 768 dim
attention_input = audio_tokens  # OK
```

### 3. Accelerator í˜¸í™˜ì„±
```python
# âŒ ë¬¸ì œ: Acceleratorì™€ optimizer prepare ì¶©ëŒ
optimizer = accelerator.prepare(optimizer)  # FP16 gradient error

# âœ… í•´ê²°: optimizerëŠ” prepareí•˜ì§€ ì•ŠìŒ
model, dataloader = accelerator.prepare(model, dataloader)
# optimizerëŠ” ë³„ë„ ê´€ë¦¬
```

## ğŸ“ ë‹¨ê³„ë³„ ìµœì í™” ê³„íš

### Phase 1: ì•ˆì „í•œ Data Pipeline ê°œì„  (ìœ„í—˜ë„: ë‚®ìŒ)
**ëª©í‘œ**: I/O ë³‘ëª© í•´ê²°, í•™ìŠµ ì†ë„ 20% í–¥ìƒ

#### 1.1 DataLoader ìµœì í™” (ê¸°ì¡´ dataset.py ìœ ì§€)
```python
# src/data/dataset_wrapper.py (ìƒˆ íŒŒì¼)
class DatasetWrapper:
    def __init__(self, original_dataset):
        self.dataset = original_dataset
        self.cache = {}  # Simple caching
    
    def __getitem__(self, idx):
        if idx not in self.cache:
            self.cache[idx] = self.dataset[idx]
        return self.cache[idx]
```

#### 1.2 ì•ˆì „í•œ Augmentation ì¶”ê°€
```python
# src/data/safe_augmentation.py
class SafeAugmentation:
    def __init__(self, enabled=False):  # ê¸°ë³¸ê°’ OFF
        self.enabled = enabled
    
    def apply(self, audio, image):
        if not self.enabled:
            return audio, image
        # ê°„ë‹¨í•œ augmentationë§Œ
        if random.random() < 0.5:
            image = torch.flip(image, dims=[-1])  # Horizontal flip only
        return audio, image
```

### Phase 2: í•™ìŠµ ì•ˆì •ì„± ê°œì„  (ìœ„í—˜ë„: ì¤‘ê°„)
**ëª©í‘œ**: NaN ë°©ì§€, í•™ìŠµ ì•ˆì •í™”

#### 2.1 Gradient Monitoring
```python
# src/utils/gradient_monitor.py
class GradientMonitor:
    def check_gradients(self, model, step):
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                if grad_norm > 100 or torch.isnan(param.grad).any():
                    print(f"Warning at step {step}: {name} grad_norm={grad_norm}")
                    return False
        return True
```

#### 2.2 Safe Mixed Precision
```python
# BF16 ê°•ì œ (FP16ë³´ë‹¤ ì•ˆì •ì )
class SafeMixedPrecision:
    def __init__(self):
        self.dtype = torch.bfloat16
        self.scaler = None  # BF16ì€ scaler ë¶ˆí•„ìš”
    
    def forward(self, model, inputs):
        with torch.autocast(device_type='cuda', dtype=self.dtype):
            return model(**inputs)
```

### Phase 3: ë©”ëª¨ë¦¬ ìµœì í™” (ìœ„í—˜ë„: ì¤‘ê°„)
**ëª©í‘œ**: ë©”ëª¨ë¦¬ 40% ì ˆì•½, ë°°ì¹˜ í¬ê¸° 2ë°°

#### 3.1 Selective Gradient Checkpointing
```python
# ì•ˆì •ì ì¸ ë ˆì´ì–´ë§Œ checkpointing
def setup_gradient_checkpointing(unet):
    # Down blocksë§Œ (ì•ˆì •ì )
    for down_block in unet.down_blocks:
        down_block.gradient_checkpointing = True
    # Mid, Up blocksëŠ” ê·¸ëŒ€ë¡œ (Stage 3 ë¬¸ì œ ë°©ì§€)
```

#### 3.2 Memory-Efficient Optimizer
```python
# 8-bit Adam (bitsandbytes)
import bitsandbytes as bnb

def create_optimizer(params, lr, use_8bit=False):
    if use_8bit:
        # Stage 1,2ë§Œ ì ìš© (Stage 3ëŠ” ì œì™¸)
        return bnb.optim.Adam8bit(params, lr=lr)
    else:
        return torch.optim.Adam(params, lr=lr)
```

### Phase 4: Stageë³„ ìµœì í™” (ìœ„í—˜ë„: ë†’ìŒ)

#### 4.1 Stage 1 ìµœì í™”
```python
# Safe Stage 1 improvements
config_stage1 = {
    "learning_rate": 1e-4,  # ê²€ì¦ëœ ê°’
    "warmup_steps": 100,    # ì§§ê²Œ
    "gradient_clip": 0.5,   # ë³´ìˆ˜ì 
    "use_ema": False,       # Stage 1ì—ì„  ë¶ˆí•„ìš”
}
```

#### 4.2 Stage 2 ìµœì í™”
```python
# LoRA rank ë™ì  ì¡°ì •
def adaptive_lora_rank(step, initial_rank=8):
    if step < 1000:
        return 4  # ì´ˆë°˜ì—” ì‘ê²Œ
    elif step < 3000:
        return 8  # ì¤‘ë°˜
    else:
        return 16  # í›„ë°˜ì—” í¬ê²Œ
```

#### 4.3 Stage 3 ìµœì í™” (íŠ¹ë³„ ì£¼ì˜!)
```python
# Stage 3ëŠ” ì´ë¯¸ ìµœì í™”ë¨ - ê±´ë“œë¦¬ì§€ ì•Šê¸°!
config_stage3 = {
    "learning_rate": 0.1,      # ê²€ì¦ëœ ê°’
    "optimizer": "Adam",        # SGD ì‹œë„ X
    "early_stopping": True,     # í•„ìˆ˜
    "target_range": (0.35, 0.45),  # ê²€ì¦ëœ ë²”ìœ„
}
```

## ğŸ”§ êµ¬í˜„ ì „ëµ

### 1. ìƒˆ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
```bash
scripts/
â”œâ”€â”€ train.py                 # ì›ë³¸ ìœ ì§€
â”œâ”€â”€ train_optimized_v1.py    # Phase 1 ì ìš©
â”œâ”€â”€ train_optimized_v2.py    # Phase 1+2 ì ìš©
â””â”€â”€ train_optimized_final.py # ì „ì²´ ì ìš©
```

### 2. ì„¤ì • íŒŒì¼ ë¶„ë¦¬
```bash
configs/
â”œâ”€â”€ training_config.json           # ì›ë³¸ ìœ ì§€
â”œâ”€â”€ training_config_safe.json      # ì•ˆì „í•œ ì„¤ì •
â””â”€â”€ training_config_optimized.json # ìµœì í™” ì„¤ì •
```

### 3. í…ŒìŠ¤íŠ¸ í”„ë¡œí† ì½œ
```python
# test_optimization.py
def test_optimization(config_path):
    # 1. 100 stepsë§Œ í…ŒìŠ¤íŠ¸
    # 2. Loss ëª¨ë‹ˆí„°ë§
    # 3. NaN ì²´í¬
    # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
    # 5. ì†ë„ ì¸¡ì •
    pass
```

## ğŸ“Š ê²€ì¦ ë©”íŠ¸ë¦­

### í•„ìˆ˜ ì²´í¬ í•­ëª©
- [ ] Lossê°€ ê°ì†Œí•˜ëŠ”ê°€?
- [ ] NaNì´ ë°œìƒí•˜ì§€ ì•ŠëŠ”ê°€?
- [ ] Gate parameterê°€ í•™ìŠµë˜ëŠ”ê°€? (Stage 3)
- [ ] ë©”ëª¨ë¦¬ ì˜¤ë²„í”Œë¡œìš°ê°€ ì—†ëŠ”ê°€?
- [ ] ì´ì „ë³´ë‹¤ ë¹ ë¥¸ê°€?

### ì„±ëŠ¥ ëª©í‘œ
- í•™ìŠµ ì†ë„: 20-30% í–¥ìƒ
- ë©”ëª¨ë¦¬ ì‚¬ìš©: 30-40% ê°ì†Œ
- í•™ìŠµ ì•ˆì •ì„±: NaN ë°œìƒ 0%
- ìµœì¢… í’ˆì§ˆ: ê¸°ì¡´ ë™ë“± ì´ìƒ

## âš ï¸ ë¡¤ë°± ê³„íš

### ë¬¸ì œ ë°œìƒ ì‹œ
1. ì¦‰ì‹œ ì›ë³¸ `train.py` ì‚¬ìš©
2. ê¸°ì¡´ `training_config.json` ë³µì›
3. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
4. ë¬¸ì œì  ë¬¸ì„œí™”

### ë°±ì—… ì „ëµ
```bash
# ì‘ë™í•˜ëŠ” ë²„ì „ ë°±ì—…
cp scripts/train.py scripts/train_stable_backup.py
cp configs/training_config.json configs/training_config_stable_backup.json
```

## ğŸš€ ì‹¤í–‰ ìˆœì„œ

### Week 1: ì•ˆì „í•œ ê°œì„ 
1. DataLoader ìºì‹± (Phase 1.1)
2. ê°„ë‹¨í•œ Augmentation (Phase 1.2)
3. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### Week 2: ì¤‘ê°„ ìœ„í—˜ ê°œì„ 
1. Gradient Monitoring (Phase 2.1)
2. BF16 ìµœì í™” (Phase 2.2)
3. Memory ìµœì í™” (Phase 3)

### Week 3: ê³ ìœ„í—˜ ê°œì„ 
1. Stageë³„ ì„¸ë¶€ íŠœë‹ (Phase 4)
2. ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸
3. ìµœì¢… ë²¤ì¹˜ë§ˆí¬

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹œì‘ ì „
- [x] í˜„ì¬ ì‘ë™í•˜ëŠ” ì½”ë“œ ë°±ì—…
- [x] íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë¬¸ì„œ ê²€í† 
- [ ] í…ŒìŠ¤íŠ¸ í™˜ê²½ ì¤€ë¹„

### ê° Phase í›„
- [ ] ì„±ëŠ¥ ì¸¡ì •
- [ ] ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
- [ ] ë¬¸ì œì  ê¸°ë¡
- [ ] ë‹¤ìŒ ë‹¨ê³„ ê²°ì •

---
*ì‘ì„±ì¼: 2025ë…„ 8ì›” 13ì¼*
*ê¸°ë°˜: TROUBLESHOOTING_REPORT.mdì˜ ëª¨ë“  ë¬¸ì œ í•´ê²° ê²½í—˜*