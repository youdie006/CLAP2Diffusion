{
  "model": {
    "base_model": "runwayml/stable-diffusion-v1-5",
    "clap_model": "laion/larger_clap_music_and_speech",
    "audio_dim": 512,
    "text_dim": 768,
    "use_adapter_list": [false, true, false],
    "hierarchy_levels": ["foreground", "full", "ambience"],
    "use_4layer_projection": true,
    "initial_gate_values": [0.5, 0.4, 0.3],
    "num_audio_tokens": 77,
    "hierarchy_config": {
      "foreground": 5,
      "background": 3,
      "ambience": 2
    }
  },
  
  "data": {
    "data_root": "/mnt/d/MyProject/CLAP2Diffusion/data/audiocaps",
    "metadata_path": "/mnt/d/MyProject/CLAP2Diffusion/data/audiocaps/metadata_validated.json",
    "batch_size": 2,
    "num_workers": 0,
    "composition_strategy": "matching",
    "max_samples": 5000,
    "audio_duration": 10.0,
    "sample_rate": 48000,
    "image_size": 512
  },
  
  "training": {
    "current_stage": 1,
    "gradient_accumulation_steps": 1,
    "mixed_precision": false,
    "max_grad_norm": 1.0,
    "use_wandb": true,
    "gradient_checkpointing": false,
    "use_lora": true,
    "lora_rank": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "seed": 42
  },
  
  "stage1": {
    "name": "LoRA + Audio Training",
    "description": "Train LoRA adapters with audio modules",
    "num_steps": 3000,
    "learning_rate": 1e-4,
    "weight_decay": 0.01,
    "trainable_modules": [
      "lora_layers",
      "audio_encoder.decomposer",
      "gates"
    ],
    "freeze_unet_base": true,
    "freeze_gates": false,
    "objectives": [
      "diffusion_loss",
      "audio_text_alignment"
    ]
  },
  
  "stage2": {
    "name": "Hierarchical Optimization",
    "description": "Optimize hierarchy weights and gated attention",
    "num_steps": 2000,
    "learning_rate": 1e-5,
    "weight_decay": 0.01,
    "trainable_modules": [
      "hierarchy_weights",
      "gates",
      "audio_projector"
    ],
    "freeze_gates": false,
    "objectives": [
      "composition_aware_loss",
      "hierarchy_balance"
    ]
  },
  
  "stage3": {
    "name": "Fine-tuning",
    "description": "Fine-tune all components together",
    "num_steps": 1000,
    "learning_rate": 5e-6,
    "weight_decay": 0.001,
    "trainable_modules": [
      "all_adapters"
    ],
    "freeze_gates": false,
    "use_lora": true,
    "lora_config": {
      "rank": 8,
      "alpha": 32,
      "dropout": 0.1,
      "target_modules": ["to_k", "to_q", "to_v", "to_out.0"]
    },
    "objectives": [
      "full_optimization"
    ]
  },
  
  "evaluation": {
    "metrics": [
      "fid_score",
      "clip_score",
      "audio_visual_alignment",
      "composition_accuracy"
    ],
    "eval_steps": 500,
    "save_samples": true,
    "num_eval_samples": 100
  },
  
  "domain_specific": {
    "enable_domain_gates": true,
    "domains": {
      "nature": {
        "gate_values": [0.3, 0.5, 0.8],
        "description": "Nature sounds - emphasize ambience"
      },
      "urban": {
        "gate_values": [0.8, 0.5, 0.3],
        "description": "Urban sounds - emphasize foreground"
      },
      "music": {
        "gate_values": [0.5, 0.7, 0.5],
        "description": "Musical content - balanced"
      },
      "speech": {
        "gate_values": [0.9, 0.3, 0.1],
        "description": "Speech - strong foreground focus"
      }
    }
  },
  
  "inference": {
    "num_inference_steps": 50,
    "guidance_scale": 7.5,
    "default_f_multiplier": 0.6,
    "composition_presets": {
      "matching": {
        "f_multiplier": 0.8,
        "description": "Strong audio-text alignment"
      },
      "complementary": {
        "f_multiplier": 0.6,
        "description": "Balanced audio-text fusion"
      },
      "creative": {
        "f_multiplier": 0.4,
        "description": "Creative interpretation"
      },
      "minimal": {
        "f_multiplier": 0.2,
        "description": "Minimal audio influence"
      }
    }
  },
  
  "hardware": {
    "device": "cuda",
    "num_gpus": 1,
    "fp16": true,
    "gradient_checkpointing": false,
    "cpu_offload": false
  },
  
  "paths": {
    "output_dir": "./outputs/v4_hybrid",
    "checkpoint_dir": "./checkpoints/v4_hybrid",
    "log_dir": "./logs/v4_hybrid",
    "sample_dir": "./samples/v4_hybrid"
  },
  
  "notes": {
    "version": "4.0-hybrid",
    "architecture": "Hierarchical Decomposition + Gated Attention",
    "key_innovation": "UNet level-specific hierarchy attention",
    "inspired_by": "SonicDiffusion + Our Hierarchical Approach",
    "expected_training_time": "6-8 hours on single GPU",
    "expected_parameters": "~5M trainable"
  }
}