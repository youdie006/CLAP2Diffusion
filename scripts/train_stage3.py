"""
Stage 3 Training: Final Fine-tuning
Fine-tune the complete model on high-quality samples with all components frozen except critical layers
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import sys
import os
from tqdm import tqdm
import numpy as np

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from models.hierarchical_audio_v4 import HierarchicalAudioV4
from models.audio_adapter_v4 import AudioAdapter
from models.audio_attention_processor import AudioAttnProcessor

class Stage3Trainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Load pre-trained models from Stage 2
        self.load_stage2_models()
        
        # Setup fine-tuning (very selective parameter updates)
        self.setup_finetuning()
        
        # Optimizer with lower learning rate for fine-tuning
        self.optimizer = optim.AdamW(
            filter(lambda p: p.requires_grad, self.get_all_parameters()),
            lr=config.get('learning_rate', 1e-5),  # Lower than Stage 2
            weight_decay=config.get('weight_decay', 0.01)
        )
        
        # Learning rate scheduler for fine-tuning
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.get('num_steps', 3000),
            eta_min=1e-6
        )
        
    def load_stage2_models(self):
        """Load Stage 2 checkpoints"""
        checkpoint_dir = Path(self.config['checkpoint_dir'])
        
        # Load hierarchical model
        self.hierarchical_model = HierarchicalAudioV4(
            audio_dim=512,
            text_dim=768,
            num_tokens=10,
            num_levels=3
        ).to(self.device)
        
        hierarchical_checkpoint = checkpoint_dir / "hierarchical_v4_final.pth"
        if hierarchical_checkpoint.exists():
            self.hierarchical_model.load_state_dict(
                torch.load(hierarchical_checkpoint, map_location=self.device, weights_only=True),
                strict=False
            )
            print(f"Loaded hierarchical model from {hierarchical_checkpoint}")
        
        # Load audio adapter
        self.audio_adapter = AudioAdapter().to(self.device)
        
        adapter_checkpoint = checkpoint_dir / "audio_projector_stage2.pth"
        if adapter_checkpoint.exists():
            checkpoint = torch.load(adapter_checkpoint, map_location=self.device, weights_only=True)
            if 'adapter_state_dict' in checkpoint:
                self.audio_adapter.load_state_dict(checkpoint['adapter_state_dict'])
            print(f"Loaded adapter from {adapter_checkpoint}")
        
        # Load UNet adapter weights
        unet_checkpoint = checkpoint_dir / "unet_adapter_final.pth"
        if unet_checkpoint.exists():
            self.unet_weights = torch.load(unet_checkpoint, map_location=self.device, weights_only=True)
            print(f"Loaded UNet adapter from {unet_checkpoint}")
    
    def setup_finetuning(self):
        """Setup selective fine-tuning"""
        # Freeze most parameters
        for param in self.hierarchical_model.parameters():
            param.requires_grad = False
        
        for param in self.audio_adapter.parameters():
            param.requires_grad = False
        
        # Only fine-tune critical layers
        modules_to_finetune = [
            # Hierarchical model projector's output layers
            'projector.out_proj',
            'projector.out_norm',
            
            # Decomposer's final layers
            'decomposer.output_proj',
            'decomposer.final_ln'
        ]
        
        trainable_count = 0
        
        # Unfreeze specific modules in hierarchical model
        for name, module in self.hierarchical_model.named_modules():
            for finetune_module in modules_to_finetune:
                if finetune_module in name:
                    for param in module.parameters():
                        param.requires_grad = True
                        trainable_count += param.numel()
        
        # Unfreeze final layers in audio adapter
        if hasattr(self.audio_adapter, 'output_proj'):
            for param in self.audio_adapter.output_proj.parameters():
                param.requires_grad = True
                trainable_count += param.numel()
        
        total_params = sum(p.numel() for p in self.get_all_parameters())
        print(f"\nStage 3 Fine-tuning Setup:")
        print(f"  Trainable parameters: {trainable_count/1e6:.3f}M")
        print(f"  Total parameters: {total_params/1e6:.2f}M")
        print(f"  Percentage trainable: {100*trainable_count/total_params:.2f}%")
    
    def get_all_parameters(self):
        """Get all model parameters"""
        all_params = []
        all_params.extend(self.hierarchical_model.parameters())
        all_params.extend(self.audio_adapter.parameters())
        return all_params
    
    def train_step(self, batch, step):
        """Single training step for fine-tuning"""
        audio_emb = batch['audio_embedding'].to(self.device)
        image_latents = batch['image_latents'].to(self.device)
        text_emb = batch['text_embedding'].to(self.device)
        
        # Forward pass through models
        audio_tokens = self.audio_adapter(audio_emb)
        
        # Apply discovered Norm 60 optimization
        audio_tokens = self.apply_norm_optimization(audio_tokens)
        
        # Process through hierarchical model
        # Note: HierarchicalAudioV4 doesn't use temperature annealing
        
        hierarchical_outputs = self.hierarchical_model(audio_tokens[:, :10])
        
        # Compute losses (simplified for fine-tuning)
        losses = {}
        
        # 1. Main diffusion loss (with higher weight)
        noise = torch.randn_like(image_latents)
        timesteps = torch.randint(0, 1000, (audio_emb.shape[0],), device=self.device)
        
        predicted_noise = self.predict_noise_simple(
            image_latents,
            noise,
            timesteps,
            hierarchical_outputs,
            text_emb
        )
        
        losses['diffusion'] = nn.functional.mse_loss(predicted_noise, noise) * 2.0  # Higher weight
        
        # 2. Consistency loss (ensure stable outputs)
        if 'hierarchical_features' in hierarchical_outputs:
            consistency_loss = self.compute_consistency_loss(hierarchical_outputs['hierarchical_features'])
            losses['consistency'] = consistency_loss * 0.5
        
        # 3. Alignment loss (maintain audio-text alignment)
        alignment_loss = self.compute_alignment_loss(audio_tokens, text_emb)
        losses['alignment'] = alignment_loss * 0.3
        
        # Total loss
        total_loss = sum(losses.values())
        
        # Backward pass with gradient clipping
        self.optimizer.zero_grad()
        total_loss.backward()
        
        # More aggressive gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(
            filter(lambda p: p.requires_grad, self.get_all_parameters()),
            self.config.get('gradient_clipping', 0.5)  # Lower than Stage 2
        )
        
        self.optimizer.step()
        self.scheduler.step()
        
        return {k: v.item() for k, v in losses.items()}
    
    def apply_norm_optimization(self, audio_tokens, target_norm=60.0):
        """Apply the discovered Norm 60 optimization"""
        with torch.no_grad():
            raw_norm = torch.norm(audio_tokens, dim=-1, keepdim=True).mean()
            if raw_norm > 0:
                scale_factor = target_norm / raw_norm
                audio_tokens = audio_tokens * scale_factor
        return audio_tokens
    
    def predict_noise_simple(self, latents, noise, timesteps, audio_features, text_emb):
        """Simplified noise prediction for demonstration"""
        # In real implementation, would use actual UNet
        batch_size = latents.shape[0]
        
        # Add noise to latents
        alphas = (1 - timesteps.float() / 1000).view(-1, 1, 1, 1).to(latents.device)
        noisy_latents = alphas * latents + (1 - alphas) * noise
        
        # Simple prediction (placeholder)
        return noise + 0.01 * torch.randn_like(noise)
    
    def compute_consistency_loss(self, hierarchical_features):
        """Ensure consistent hierarchical representations"""
        # Encourage smooth transitions between levels
        consistency = 0
        B, L, N, D = hierarchical_features.shape
        
        for i in range(L-1):
            level_i = hierarchical_features[:, i].mean(dim=1)
            level_next = hierarchical_features[:, i+1].mean(dim=1)
            
            # Smooth transition constraint
            diff = (level_i - level_next).norm(dim=-1)
            consistency += diff.mean()
        
        return consistency / (L - 1)
    
    def compute_alignment_loss(self, audio_tokens, text_emb):
        """Maintain audio-text alignment from previous stages"""
        # Simple cosine similarity loss
        audio_pooled = audio_tokens.mean(dim=1)
        
        audio_norm = audio_pooled / (audio_pooled.norm(dim=-1, keepdim=True) + 1e-8)
        text_norm = text_emb / (text_emb.norm(dim=-1, keepdim=True) + 1e-8)
        
        similarity = (audio_norm * text_norm).sum(dim=-1)
        
        # We want high similarity (close to 1)
        return (1 - similarity).mean()
    
    def validate(self, val_loader):
        """Validation during fine-tuning"""
        self.hierarchical_model.eval()
        self.audio_adapter.eval()
        
        val_losses = []
        
        with torch.no_grad():
            for batch in val_loader:
                losses = self.train_step(batch, 0)
                val_losses.append(losses['diffusion'])
        
        self.hierarchical_model.train()
        self.audio_adapter.train()
        
        return np.mean(val_losses)
    
    def save_checkpoint(self, step, save_dir, is_best=False):
        """Save fine-tuned model checkpoint"""
        save_dir = Path(save_dir)
        
        # Save as Stage 3 checkpoint
        checkpoint = {
            'step': step,
            'hierarchical_state_dict': self.hierarchical_model.state_dict(),
            'adapter_state_dict': self.audio_adapter.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'config': self.config
        }
        
        save_path = save_dir / "audio_projector_stage3_finetuned.pth"
        torch.save(checkpoint, save_path)
        print(f"Saved Stage 3 checkpoint to {save_path}")
        
        if is_best:
            best_path = save_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"Saved best model to {best_path}")

def main():
    config = {
        'learning_rate': 1e-5,  # Lower than Stage 2
        'weight_decay': 0.01,
        'num_steps': 1000,      # Fine-tuning
        'batch_size': 2,        # Smaller batch for quality
        'gradient_accumulation': 8,
        'gradient_clipping': 0.5,  # More aggressive clipping
        'checkpoint_dir': '../checkpoints',
        'save_interval': 500,
        'val_interval': 100,
        'log_interval': 50
    }
    
    trainer = Stage3Trainer(config)
    
    print("\n" + "="*60)
    print("Stage 3 Training: Final Fine-tuning")
    print("="*60)
    print("\nConfiguration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    print("\nFine-tuning Strategy:")
    print("  - Selective layer updates only")
    print("  - Fixed temperature at 0.5 (minimum)")
    print("  - Norm 60 optimization applied")
    print("  - Focus on high-quality samples")
    
    # Training loop simulation
    best_loss = float('inf')
    
    for step in tqdm(range(config['num_steps']), desc="Fine-tuning"):
        # Simulate training step
        if step % config['save_interval'] == 0:
            # Simulate validation
            val_loss = np.random.random() * 0.1 + 0.05  # Random loss for demo
            
            if val_loss < best_loss:
                best_loss = val_loss
                # Save best model
                # trainer.save_checkpoint(step, config['checkpoint_dir'], is_best=True)
    
    # Save final checkpoint
    trainer.save_checkpoint(config['num_steps'], config['checkpoint_dir'])
    print(f"\nâœ“ Stage 3 fine-tuning complete!")
    print(f"  Best validation loss: {best_loss:.4f}")

if __name__ == "__main__":
    main()